<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Linear Regression Visualizing between 2 or more variables  Scatterplot - the position of each point is determined by the values of two numerical variables: one on the horizontal (x) axis, the other on the vertical (y) axis (one point for each observation)  Features of the association between the two numerical variables   Form - describes the pattern that the two variables follow together
 Ex. Linear, non-linear, quadratic, exponential&mldr;"><title>STA130: Lecture_7</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://mel10c.github.io/mel.zhu//icon.png><link href=https://mel10c.github.io/mel.zhu/styles.9a8661985b360a1d97e0e538e164abef.min.css rel=stylesheet><link href=https://mel10c.github.io/mel.zhu/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://mel10c.github.io/mel.zhu/js/darkmode.905e4d2da56a9111aff695a0a4b69900.min.js></script>
<script src=https://mel10c.github.io/mel.zhu/js/util.6f22941e242efae60fd84e7c32e874fa.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script async src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script async src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script async src=https://mel10c.github.io/mel.zhu/js/popover.f03552ccb84d99ca615d1cfb9abde59e.min.js></script>
<script defer src=https://mel10c.github.io/mel.zhu/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://mel10c.github.io/mel.zhu/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://mel10c.github.io/mel.zhu/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://mel10c.github.io/mel.zhu/",fetchData=Promise.all([fetch("https://mel10c.github.io/mel.zhu/indices/linkIndex.4ecd462b2a0cc7b8f254e52a69d96e3c.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://mel10c.github.io/mel.zhu/indices/contentIndex.6acfc2f415caa35851b6b3067c8415cf.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://mel10c.github.io/mel.zhu",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://mel10c.github.io/mel.zhu",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'â€™':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/mel10c.github.io\/mel.zhu\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://mel10c.github.io/mel.zhu/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://mel10c.github.io/mel.zhu/>MEL.ZHU</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>STA130: Lecture_7</h1><p class=meta>Last updated
Unknown</p><ul class=tags><li><a href=https://mel10c.github.io/mel.zhu/tags/note/>Note</a></li></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#visualizing-between-2-or-more-variables>Visualizing between 2 or more variables</a><ol><li><a href=#features-of-the-association-between-the-two-numerical-variables>Features of the association between the two numerical variables</a></li><li><a href=#visualizing-the-numerical-variable-with-categorical-variable>Visualizing the numerical variable with categorical variable</a></li></ol></li><li><a href=#quantifying-association-correlation>Quantifying association: correlation</a></li><li><a href=#linear-regression-models>Linear Regression Models</a><ol><li><a href=#1-numerical-predictor>1: Numerical predictor</a></li><li><a href=#2-categorical-predictor>2: Categorical predictor</a></li></ol></li><li><a href=#inference-for-simple-linear-regression>Inference for Simple Linear Regression</a><ol><li><a href=#assumptions-for-statistical-inference-on-regression-coefficients>Assumptions for statistical inference on regression coefficients</a></li><li><a href=#using-r-for-hypothesis-testing>Using <code>R</code> for hypothesis testing</a></li></ol></li></ol></nav></details></aside><a href=#linear-regression><h1 id=linear-regression><span class=hanchor arialabel=Anchor># </span>Linear Regression</h1></a><a href=#visualizing-between-2-or-more-variables><h2 id=visualizing-between-2-or-more-variables><span class=hanchor arialabel=Anchor># </span>Visualizing between 2 or more variables</h2></a><ul><li><u>Scatterplot</u> - the <strong>position</strong> of each point is determined by the values of two numerical variables: one on the horizontal (x) axis, the other on the vertical (y) axis (one point for each observation)</li></ul><a href=#features-of-the-association-between-the-two-numerical-variables><h3 id=features-of-the-association-between-the-two-numerical-variables><span class=hanchor arialabel=Anchor># </span>Features of the association between the two numerical variables</h3></a><ul><li><p><strong><u>Form</u></strong> - describes the pattern that the two variables follow together</p><blockquote><p>Ex. Linear, non-linear, quadratic, exponential&mldr;</p></blockquote></li><li><p><strong><u>Direction</u></strong></p><ul><li><em><strong>Positive association</strong></em> - values of one variable tend to increase as the other&rsquo;s increase</li><li><em><strong>Negative association</strong></em> - values of one variable tend to decrease as the other&rsquo;s increase</li></ul></li><li><p><strong><u>Strength</u></strong> - describes how concentrated the values of the variable are around the <em>pattern</em></p><ul><li>Strong, moderate, weak</li></ul></li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-R data-lang=R><span class=line><span class=cl><span class=o>&lt;</span><span class=n>data_set</span><span class=o>&gt;</span> <span class=o>%&gt;%</span> <span class=nf>ggplot</span><span class=p>(</span><span class=nf>aes</span><span class=p>(</span><span class=n>x</span><span class=o>=&lt;</span><span class=n>variable_1</span><span class=o>&gt;</span><span class=p>,</span> <span class=n>y</span><span class=o>=&lt;</span><span class=n>variable_2</span><span class=o>&gt;</span><span class=p>))</span> <span class=o>+</span> 
</span></span><span class=line><span class=cl>	<span class=nf>geom_point</span><span class=p>()</span> <span class=o>+</span>
</span></span><span class=line><span class=cl>	<span class=nf>labs</span><span class=p>(</span><span class=n>x</span> <span class=o>=</span> <span class=s>&#34;Name of x-axis (unit)&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>         <span class=n>y</span> <span class=o>=</span> <span class=s>&#34;Name of y-axis (unit)&#34;</span><span class=p>)</span> <span class=o>+</span>
</span></span><span class=line><span class=cl>	<span class=nf>theme_minimal</span><span class=p>()</span>  <span class=c1># optional, makes the backgroun white</span>
</span></span></code></pre></td></tr></table></div></div><a href=#example-of-the-heights-dataset><h4 id=example-of-the-heights-dataset><span class=hanchor arialabel=Anchor># </span>Example of the <code>heights</code> dataset</h4></a><ul><li><code>heights</code> - the name of the dataset in use (&lt;data_set>)</li><li><code>shoePrint</code> - the name of the numerical variable you want to display on the x-axis (&lt;variable_1>)</li><li><code>height</code> - the name of the numerical variable you want to display on the y-axis (&lt;variable_2>)</li></ul><a href=#visualizing-the-numerical-variable-with-categorical-variable><h3 id=visualizing-the-numerical-variable-with-categorical-variable><span class=hanchor arialabel=Anchor># </span>Visualizing the numerical variable with categorical variable</h3></a><ol><li><p>Add <code>ggplot(aes(... color = &lt;varaible_3>))</code> displays each point based on the value of the categorial variable</p><blockquote><p>Ex. <code>ggplot(aes(... color = sex))</code>
<img src=https://tva1.sinaimg.cn/large/008eGmZEly1gofbhw7hboj30te0meq73.jpg alt="Screen Shot 2021-03-10 at 12.20.42" style=zoom:20%></p></blockquote></li><li><p>Use <code>facet_wrap()</code>, and specify the name of a categorial variable, then would get a separate plot for each value of this variable (2 cases) (Can create side-by-side histograms or barplots&mldr;)</p><blockquote><p>Ex. <code>facet_wrap(~sex)</code>
<img src=https://tva1.sinaimg.cn/large/008eGmZEly1gofbhy1729j30y80pitef.jpg alt="Screen Shot 2021-03-10 at 12.20.47" style=zoom:20%></p></blockquote></li></ol><ul><li>Its a good idea to try both cases and see which one is more effective representation</li></ul><a href=#quantifying-association-correlation><h2 id=quantifying-association-correlation><span class=hanchor arialabel=Anchor># </span>Quantifying association: correlation</h2></a><ul><li><p><u>Correlation</u> summarizes the <strong>strength</strong> and <strong>direction</strong> of the <strong>linear</strong> relationship between two numerical variables (non-linear relationship is not represented in correlation)</p></li><li><p><u>Sample correlation</u> between variables $x, y$ for $n$ observations $(x_1, y_1) \dots (x_n, y_n)$:
$$
r = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}
{\sum_{i=1}^n (x_i - \bar{x})^2 \cdot \sum_{i=1}^n (y_i - \bar{y})^2}
$$</p></li><li><p>The <em>sign</em> of $r$ gives the <strong>direction</strong> ($r>0$ - positive, $r&lt;0$ - negative)</p><ul><li>The <em>magnitude</em> of $r$ is a measure of the <strong>strength of the leaner association</strong><ul><li>$-1 \le r \le 1$</li><li>$r = \pm 1$ If and only if there is a <em>perfect</em> linear relationship between $x$ and $y$</li></ul></li></ul></li></ul><blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-R data-lang=R><span class=line><span class=cl><span class=nf>cor</span><span class=p>(</span><span class=n>x</span> <span class=o>=</span> <span class=n>heights</span><span class=o>$</span><span class=n>shoePrint</span><span class=p>,</span>  <span class=c1># &#39;$&#39; exrtract a column vector from a tibble</span>
</span></span><span class=line><span class=cl> 	<span class=n>y</span> <span class=o>=</span> <span class=n>heights</span><span class=o>$</span><span class=n>height</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>## [1] 0.812948</span>
</span></span></code></pre></td></tr></table></div></div><p>The correlation between shoe print length and height in this sample of 40 individuals is 0.081</p></blockquote><a href=#linear-regression-models><h2 id=linear-regression-models><span class=hanchor arialabel=Anchor># </span>Linear Regression Models</h2></a><a href=#1-numerical-predictor><h3 id=1-numerical-predictor><span class=hanchor arialabel=Anchor># </span>1: Numerical predictor</h3></a><ul><li><p>Goal of linear regression: understand variation, predict pattern (both needs a <strong>model</strong>)</p></li><li><p><u><a class="internal-link broken">Simple linear regression</a> model</u> - assumes there is a &ldquo;best&rdquo; straight line that explains the real relationship between $x$ and $y$ and that the values observed randomly deviate from this line
$$
Y_i = (\beta_0 + \beta_1 x_i) + \epsilon_i
$$</p></li><li><p>$Y_1$ - response variable (or dependent variable, target variable&mldr;) for observation $i$</p><ul><li>Values are random, and observed in the sample data</li></ul></li><li><p>$x_i$ - independent variable (or predictor, covariate, feature, input&mldr;) for observation $i$</p><ul><li>Fixed (constant) and observed in sample data</li></ul></li><li><p>$\beta_0$ - intercept parameter (closed-form math expression for estimated regression coefficient)
$$
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}
$$</p></li><li><p>$\beta_1$ - slope parameter (closed-form math expression for estimated regression coefficient)
$$
\hat{\beta}<em>1 =
\frac{\sum</em>{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}
{\sum_{i=1}^n (x_i - \tilde{x})^2}
$$</p><ul><li>Both $\beta$ are fixed (constants) but unknown</li></ul></li><li><p>$\epsilon_i$ - random error term for observation $i$ (random deviation)</p><ul><li>Random, but cannot be calculated directly (don&rsquo;t know true value of $\beta_0, \beta_1$)</li><li>$\epsilon_i = y_i - \hat{y}_i$</li></ul></li><li><p><em>Population</em> regression is unknown, need to <strong>estimate</strong> a line in which is as close as possible to as many points as possible in the sample</p><ul><li>Most common approach is to define <em>the sum of squared vertical differences between each observation and the fitted (estimated) line</em></li><li><u>Least Squares Regression Line</u> - the straight line which <em>minimizes</em> the sum of squared vertical distances between each point and the fitted line (compared to all other possible straight lines)</li></ul></li><li><p>Use <code>lm()</code> function to fit a linear regression model</p><ul><li><img src=https://tva1.sinaimg.cn/large/008eGmZEly1gofavwv52yj30gy032dg4.jpg alt="Screen Shot 2021-03-07 at 02.13.07" style=zoom:50%></li></ul><blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-R data-lang=R><span class=line><span class=cl><span class=n>model1</span> <span class=o>&lt;-</span> <span class=nf>lm</span><span class=p>(</span><span class=n>height</span> <span class=o>~</span> <span class=n>shoePrint</span><span class=p>,</span> <span class=n>data</span><span class=o>=</span> <span class=n>heights</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nf>summary</span><span class=p>(</span><span class=n>model1</span><span class=p>)</span><span class=o>$</span><span class=n>coefficients</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>##				Estimate	Std. Error	t-value		Pr(&gt;|t|)</span>
</span></span><span class=line><span class=cl><span class=c1>## (Intercept)	80.930409	10.8933945	7.429310	6.504368e-09</span>
</span></span><span class=line><span class=cl><span class=c1>## shoePrint	3.218561	0.3740081	8.605591	1.863474e-10</span>
</span></span></code></pre></td></tr></table></div></div><ul><li><code>(Intercept)</code> is the <strong>estimate</strong> of $\beta_0$ (ie, $\hat{\beta}_0$)</li><li><code>shoePrint</code> is the <strong>estimated</strong> of $\beta_1$ (ie. $\hat{\beta}_1$)</li></ul></blockquote></li><li><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-R data-lang=R><span class=line><span class=cl><span class=nf>geom_smooth</span><span class=p>(</span><span class=n>method</span><span class=o>=</span><span class=s>&#34;lm&#34;</span><span class=p>,</span> <span class=n>se</span><span class=o>=</span><span class=kc>FALSE</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>To add the fitted regression line to a plot</li><li><img src=https://tva1.sinaimg.cn/large/008eGmZEly1goboskfhatj30j20fqdhz.jpg alt="Screen Shot 2021-03-07 at 08.59.05" style=zoom:30%></li></ul></li></ul><a href=#interpretation-of-regression-coefficients><h4 id=interpretation-of-regression-coefficients><span class=hanchor arialabel=Anchor># </span>Interpretation of regression coefficients</h4></a><ul><li><p>The estimated simple regression of $y$ on $x$ is: (<u>fitted regression line</u> or <u>fitted regression equation</u>)
$$
\hat{y} = \hat{\beta}_0 + \hat{\beta}_1x
$$</p></li><li><p>$\hat{y}$ Is the <u>fitted value</u> or the <u>predicted value</u> is the estimated average value of $y$ when the predictor is equal to $x$</p><ul><li>The <em>slope</em> $\hat{\beta}_1$ is the average change in $y$ for 1-unit change in $x$</li><li>The <em>intercept</em> $\hat{\beta}_0$ is the average of $y$ when $x=0$ (often this doesn&rsquo;t make sense, but it tells use the height of the line)</li><li>The difference between the observed and predicted value of $y$ for the $i^{th}$ observation is called the <u>residual</u> ($e_i = y_i - \hat{y}_i$) [distance between the point and the estimated line]</li></ul></li><li><p><strong>In general, it is not ok to say that a change in the predictor $x$ <em>causes</em> a change in $y$. It is only ok to talk about the association observed</strong></p><blockquote><a href=#suppose-you-wake-up-to-find-that-your-bike-has-been-stolen-but-there-is-a-fresh-shoeprint-in-the-mud-nearby-you-measure-it-and-it-is-30cm-long-based-on-this-fitted-regression-model-how-tall-would-you-predict-that-the-person-who-left-the-shoeprint-was><h5 id=suppose-you-wake-up-to-find-that-your-bike-has-been-stolen-but-there-is-a-fresh-shoeprint-in-the-mud-nearby-you-measure-it-and-it-is-30cm-long-based-on-this-fitted-regression-model-how-tall-would-you-predict-that-the-person-who-left-the-shoeprint-was><span class=hanchor arialabel=Anchor># </span>Suppose you wake up to find that your bike has been stolen, but there is a fresh shoeprint in the mud nearby. You measure it and it is 30cm long. Based on this fitted regression model, how tall would you predict that the person who left the shoeprint was?</h5></a><p>General equation for the fitted line: ($\ref{ref3}$)
$$
\begin{align*}
\hat{\text{height}} &= (\text{intercept }cm) + (\text{slope}) \cdot
(\text{length of shoeprint }cm) \\ \hat{y} &= 80.930409 cm + 3.218561 \times (30 cm) \\ \hat{y} &= 177.48cm
\end{align*}
$$</p></blockquote></li><li><p><u>Extrapolation</u> - means trying to predict the response variable for values of the explanatory variable beyond those contained in the data</p><ul><li>A model is only as good as the data it was trained on</li><li>NO reason to think that the trend for the observed range would be valid outside of the range</li></ul></li><li><p>The coefficient of determination ($R^2$) is the proportion of the variability in $y$ which is explain by the fitted regression model
$$
R^2 = \frac{\sum_{i=1}^n (y_i - \bar{y})^2 - \sum_{i=1}^n (y_i - \hat{y}^2_i)}
{\sum_{i=1}^n (y_i - \bar{y})^2} =
1 - \frac{\sum_{i=1}^n (y_i - \hat{y}<em>i)}{\sum</em>{i=1}^n (y_i - \bar{y})^2}
$$</p><ul><li>$R^2$ close to 1 indicates that most of the variability in $y$ is explain by the regression model</li><li>$R^2$ close to 0 indicates that very little of the variability in $y$ is explained by the regression model</li><li>Conveniently $R^2$ is equal to the square of the correlation $r$ ($\ref{ref1}$)</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-R data-lang=R><span class=line><span class=cl><span class=nf>summary</span><span class=p>(</span><span class=n>model1</span><span class=p>)</span><span class=o>$</span><span class=n>r.squared</span>
</span></span><span class=line><span class=cl><span class=c1>## [1] 0.6608845</span>
</span></span><span class=line><span class=cl><span class=nf>cor</span><span class=p>(</span><span class=n>x</span> <span class=o>=</span> <span class=n>height</span><span class=o>$</span><span class=n>shoePrint</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>heights</span><span class=o>$</span><span class=n>height</span><span class=p>)</span><span class=n>^2</span>
</span></span><span class=line><span class=cl><span class=c1>## [1] 0.6608845</span>
</span></span></code></pre></td></tr></table></div></div></li></ul><a href=#2-categorical-predictor><h3 id=2-categorical-predictor><span class=hanchor arialabel=Anchor># </span>2: Categorical predictor</h3></a><ul><li>For the equation for a simple regression line with one numerical predictor: ($\ref{ref2}$), for categorical data, the $x_i$ value needs a <u>indicator variable</u> to encode the categorial data
$$
x_i = I(\text{individual $i$ is male}) =
\left{\begin{matrix}
1 & \text{if }individual~ i~ is male \\ 0 & \text{if }individual~ i~ is female
\end{matrix}\right.
$$
This also needs a <u>baseline value</u> (the level corresponding to $x=0$), here F(female) is the value</li></ul><blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-R data-lang=R><span class=line><span class=cl><span class=n>model2</span> <span class=o>&lt;-</span> <span class=nf>lm</span><span class=p>(</span><span class=n>height</span> <span class=o>~</span> <span class=n>sex</span><span class=p>,</span> <span class=n>data</span><span class=o>=</span> <span class=n>heights</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nf>summary</span><span class=p>(</span><span class=n>model2</span><span class=p>)</span><span class=o>$</span><span class=n>coefficients</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>##				Estimate	Std. Error	t-value		Pr(&gt;|t|)</span>
</span></span><span class=line><span class=cl><span class=c1>## (Intercept)	166.82381	1.357760	122.866909	5.085412e-51</span>
</span></span><span class=line><span class=cl><span class=c1>## sexM			15.79198	1.970046	8.016048	1.085391e-09</span>
</span></span></code></pre></td></tr></table></div></div><ul><li><code>(Intercept)</code> is the <strong>estimate</strong> of $\beta_0$ (ie, $\hat{\beta}_0$)</li><li><code>sexM</code> is the <strong>estimated</strong> of $\beta_1$ (ie. $\hat{\beta}_1$)</li></ul></blockquote><a href=#intrerpreting-hatbeta_0-and-hatbeta_1><h4 id=intrerpreting-hatbeta_0-and-hatbeta_1><span class=hanchor arialabel=Anchor># </span>Intrerpreting $\hat{\beta}_0$ and $\hat{\beta}_1$</h4></a><ul><li>Combining the equation ($\ref{ref3}$) and equation ($\ref{ref4}$), get:<ul><li>When $x=0$, have $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 \times 0 = \hat{\beta}_0$, implies this is the predicted value of $y$ for individuals with $x=0$ (in ex. $\hat{\beta}_0$ Is the predicted height for women)</li><li>When $x=1$, have $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 \times 1 = \hat{\beta}_0 + \hat{\beta}_1$, implies that this is the predicted value of $y$ for individuals with $x=1$</li><li>$\hat{\beta}_1$ Is the average <em>differences</em> in the response variable $y$ between the two categories</li></ul></li></ul><a href=#inference-for-simple-linear-regression><h2 id=inference-for-simple-linear-regression><span class=hanchor arialabel=Anchor># </span>Inference for Simple Linear Regression</h2></a><ul><li><p>Fitted regression line: $\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 x$ (similar to $\ref{ref3}$)</p></li><li><p>However, the estimates $(\hat{\beta}_0, \hat{\beta}_1)$ does not equal to the Tre parameter values $(\beta_0, \beta_1)$</p><ul><li>The estimation is based on the sample data, so they are subject tot sampling variability</li></ul></li><li><p>For the linear model ($\ref{ref2}$), write a pair of hypotheses to test if the slope parameter in this regression model is different from 0</p><ul><li>$H_0: \beta_1 = 0$ vs $H_A: \beta_1 \neq 0$</li><li>Under the linear model, if the slope parameter is 0, then $x$ does not predict $y$, then predicting the sample mean of $y$ for all observation is fine; If the slope ($\beta_1$) is different from 0, then knowing $x$ does help to better predict $y$</li></ul></li></ul><a href=#assumptions-for-statistical-inference-on-regression-coefficients><h3 id=assumptions-for-statistical-inference-on-regression-coefficients><span class=hanchor arialabel=Anchor># </span>Assumptions for statistical inference on regression coefficients</h3></a><blockquote><p>In STA130, these assumptions did not need to be verified before doing the inference</p></blockquote><ul><li><p>The p-values in the output of <code>lm()</code> is based on the <em>Student&rsquo;s distribution</em> (a continuous probability distribution). So for this to be value, need to make a few assumptions</p><ol><li>There is a linear association between $x,y$</li><li>Constant variance in $y$ for all values of $x$ (scatterplot is not cone-shaped)</li><li>The observations are independent</li><li>The residual follow a normal distribution</li></ol></li><li><p>If one or more assumptions above is not reasonable, then the inference may not be valid</p></li></ul><a href=#using-r-for-hypothesis-testing><h3 id=using-r-for-hypothesis-testing><span class=hanchor arialabel=Anchor># </span>Using <code>R</code> for hypothesis testing</h3></a><ul><li><p><code>R</code> auto gives the p-value for hypothesis test of the form $H_0: \beta_1 = 0$ vs $H_A: \beta_1 \neq 0$</p><ul><li>P-value is the <code>Pr(>|t|)</code> in the code above (view code for <code>model1</code>)</li></ul><blockquote><p>In the example for <code>model1</code>, the p-value is very small (p-value &lt; $1.86 \times 10^{-10}$), then this indicates there is <em>very</em> strong evidence gains the null hypothesis.</p></blockquote></li></ul><hr><ul><li>Advantage of randomization test<ul><li>NO assumptions about the distribution of the data</li><li>More flexible (can be used to compare any statistic across two groups, [not just mean])</li></ul></li><li>Advantage of linear regression approach<ul><li>Only requires 1 (or 2) lines of code (but only valid <em>if</em> the assumptions are valid)</li></ul></li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script async src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://mel10c.github.io/mel.zhu/js/graph.2d9e48dbe7ea47c0ef1c58296ce14448.js></script></div></div><div id=contact_buttons><footer><p>Made by Melaney Zhu using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2023</p><ul><li><a href=https://mel10c.github.io/mel.zhu/>Home</a></li><li><a href=https://www.instagram.com/melaney_dxl/>Instagram</a></li><li><a href=https://github.com/mel10c>GitHub</a></li><li><a href=https://linkedin.com/in/melzyy>LinkedIn</a></li><li><a href=https://github.com/mel10c/mel.zhu/blob/hugo/content/documents/Resume.pdf>Resume</a></li></ul></footer></div></div></body></html>